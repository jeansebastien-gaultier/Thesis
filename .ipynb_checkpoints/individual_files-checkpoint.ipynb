{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a744d071-a22e-4d77-b32f-ef60d548968e",
   "metadata": {},
   "source": [
    "# Text Cleaning (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5b4cfb69-a1ce-427d-a3a7-66b634493d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "from xml.dom import minidom\n",
    "import convert_to_file as cf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b063db7f-d9d7-402a-9ee5-0d673419fb89",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 1 - Create a new folder with all text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e5d3090-ec92-49c9-bec1-bdc64849d5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2006\n",
    "root_06 = \"Data/2006\"\n",
    "list_of_txt_06 = functions.search_for_files(root_06, \".xml\")\n",
    "\n",
    "EHR_06 = {}\n",
    "for item in list_of_txt_06:\n",
    "    file = minidom.parse(item)\n",
    "    text = file.getElementsByTagName(\"TEXT\")\n",
    "    ID = file.getElementsByTagName(\"RECORD\")\n",
    "    for txt, idx in zip(text, ID):\n",
    "        ls = []\n",
    "        for element in txt.childNodes:\n",
    "            if type(element).attributes == None:\n",
    "                ls.append(element.data)\n",
    "            else:\n",
    "                ls.append(element.firstChild.data)\n",
    "        EHR_06[f'06_{int(idx.attributes[\"ID\"].value)}'] = \"\".join(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aadf737-32ea-4af5-843f-433b3e05d36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2008\n",
    "root_08 = \"Data/2008\"\n",
    "list_of_txt_08 = functions.search_for_files(root_08, \".xml\")\n",
    "\n",
    "EHR_08 = {}\n",
    "\n",
    "for item in list_of_txt_08:\n",
    "    file = minidom.parse(item)\n",
    "    text = file.getElementsByTagName(\"text\")\n",
    "    ID = file.getElementsByTagName(\"doc\")\n",
    "    for txt, idx in zip(text, ID):\n",
    "        # print(idx.attributes['id'].value,)\n",
    "        EHR_08[f\"08_{idx.attributes['id'].value}\"] = txt.firstChild.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66910376-03ee-410c-a593-6b70e1b66a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2009\n",
    "root_09_bis = \"Data/2009_Bis\"\n",
    "list_of_txt_09_bis = functions.search_for_files(root_09_bis, \".txt\")\n",
    "\n",
    "EHR_09_bis = {}\n",
    "\n",
    "for file in list_of_txt_09_bis:\n",
    "    with open(file, encoding=\"utf-8-sig\") as f:\n",
    "        lines = f.readlines()\n",
    "    ID = lines[0].split(\" \")[1][1:-1]\n",
    "    EHR_09_bis[f\"09_{ID}\"] = \"\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9919ad77-7d9d-44d8-b9a0-37a0ee485b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2010\n",
    "root_10 = \"Data/2010\"\n",
    "list_of_txt_10 = functions.search_for_files(root_10, \".txt\")\n",
    "\n",
    "EHR_10 = {}\n",
    "\n",
    "for file in list_of_txt_10:\n",
    "    with open(file, encoding=\"utf-8-sig\") as f:\n",
    "        lines = f.readlines()\n",
    "    ID = file.split(\"/\")[-1][:-4]\n",
    "    EHR_10[f\"10_{ID}\"] = \"\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c8e7110-d0f1-4e65-bf3b-43a76718693a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2011\n",
    "root_11 = \"Data/2011\"\n",
    "list_of_txt_11 = functions.search_for_files(root_11, \".txt\")\n",
    "\n",
    "EHR_11 = {}\n",
    "\n",
    "for file in list_of_txt_11:\n",
    "    with open(file, encoding=\"utf-8-sig\") as f:\n",
    "        lines = f.readlines()\n",
    "    ID = file.split(\"/\")[-1].split(\"-\")[-1][:-4]\n",
    "    EHR_11[f\"11_{ID}\"] = \"\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c482aaf-9790-4b62-9afa-fefd2aa07d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2012\n",
    "root_12 = \"Data/2012\"\n",
    "list_of_txt_12 = functions.search_for_files(root_12, \".txt\")\n",
    "\n",
    "EHR_12 = {}\n",
    "\n",
    "for file in list_of_txt_12:\n",
    "    with open(file, encoding=\"utf-8-sig\") as f:\n",
    "        lines = f.readlines()\n",
    "    ID = file.split(\"/\")[-1].split(\".\")[0]\n",
    "    EHR_12[f\"12_{ID}\"] = \"\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd4a876a-ad90-4e8c-963e-7e52e0c5c25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2014\n",
    "root_14 = \"Data/2014\"\n",
    "list_of_txt_14 = functions.search_for_files(root_14, \".xml\")\n",
    "\n",
    "EHR_14 = {}\n",
    "\n",
    "for item in list_of_txt_14:\n",
    "    file = minidom.parse(item)\n",
    "    text = file.getElementsByTagName(\"TEXT\")\n",
    "    for txt in text:\n",
    "        ID = item.split(\"/\")[-1][:-4]\n",
    "        EHR_14[f\"14_{ID}\"] = txt.firstChild.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "234ecc45-0b6e-4342-b750-3de276273af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2018_1\n",
    "root_18_1 = \"Data/2018_1\"\n",
    "list_of_txt_18_1 = functions.search_for_files(root_18_1, \".xml\")\n",
    "\n",
    "EHR_18_1 = {}\n",
    "\n",
    "for files in list_of_txt_18_1:\n",
    "    i = 0\n",
    "    fls = []\n",
    "    file = minidom.parse(files)\n",
    "    text = file.getElementsByTagName(\"TEXT\")\n",
    "    for txt in text:\n",
    "        lg_text = txt.firstChild.data\n",
    "        lg_text = lg_text.split(\"Record date\")\n",
    "        for item in lg_text:\n",
    "            ID = files.split(\"/\")[-1][:-4]\n",
    "            EHR_18_1[f\"18_1_{ID}_{i}\"] = \"Record date\" + item\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3c0c0b32-fec3-49af-924f-a2991f07a090",
   "metadata": {},
   "outputs": [],
   "source": [
    "EHR_18_1_c = EHR_18_1.copy()\n",
    "for k, txt in EHR_18_1.items():\n",
    "    if txt == \"Record date\\n\\n\":\n",
    "        del EHR_18_1_c[k]\n",
    "\n",
    "EHR_18_1 = EHR_18_1_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7c19ea9a-dc06-49db-b8f7-834c55f69956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2018_2\n",
    "root_18_2 = \"Data/2018_2\"\n",
    "list_of_txt_18_2 = functions.search_for_files(root_18_2, \".txt\")\n",
    "\n",
    "EHR_18_2 = {}\n",
    "\n",
    "for file in list_of_txt_18_2:\n",
    "    with open(file, encoding=\"utf-8-sig\") as f:\n",
    "        lines = f.readlines()\n",
    "    ID = file.split(\"/\")[-1][:-4]\n",
    "    EHR_18_2[f\"18_2_{ID}\"] = \"\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8e477275-f1ce-46d9-b8f1-cf0fbeac2967",
   "metadata": {},
   "outputs": [],
   "source": [
    "EHR_years = {\n",
    "    \"06\": EHR_06,\n",
    "    \"08\": EHR_08,\n",
    "    \"09\": EHR_09_bis,\n",
    "    \"10\": EHR_10,\n",
    "    \"11\": EHR_11,\n",
    "    \"12\": EHR_12,\n",
    "    \"14\": EHR_14,\n",
    "    \"18_1\": EHR_18_1,\n",
    "    \"18_2\": EHR_18_2,\n",
    "}\n",
    "# for _, year in EHR_years.items():\n",
    "#     for idx, txt in year.items():\n",
    "#         if not os.path.exists(f'Data/ALL/{idx}.txt'):\n",
    "#             cf.new_file('Data/ALL',idx,txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "16ea20e2-8c69-4313-8381-3661a191f123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>06_502</th>\n",
       "      <td>\\n113416550\\nPRGH\\n13523357\\n630190\\n6/7/1999 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>06_503</th>\n",
       "      <td>\\n229937784\\nFIH\\n8765992\\n87919/3984\\n754719\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>06_504</th>\n",
       "      <td>\\n010760828\\nFIH\\n4604997\\n76732/99w3\\n503426\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>06_505</th>\n",
       "      <td>\\n333145593\\nFIH\\n9229891\\n27487/f4b3\\n699211\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>06_506</th>\n",
       "      <td>\\n333145593\\nFIH\\n9229891\\n50096/c65m\\n571634\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18_2_123244</th>\n",
       "      <td>Admission Date:  [**2100-11-30**]             ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18_2_107058</th>\n",
       "      <td>Admission Date:  [**2178-3-12**]              ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18_2_114923</th>\n",
       "      <td>Admission Date:  [**2132-1-30**]              ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18_2_103074</th>\n",
       "      <td>Admission Date:  [**2125-10-10**]       Discha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18_2_101276</th>\n",
       "      <td>Admission Date:  [**2120-10-4**]              ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7901 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             0\n",
       "06_502       \\n113416550\\nPRGH\\n13523357\\n630190\\n6/7/1999 ...\n",
       "06_503       \\n229937784\\nFIH\\n8765992\\n87919/3984\\n754719\\...\n",
       "06_504       \\n010760828\\nFIH\\n4604997\\n76732/99w3\\n503426\\...\n",
       "06_505       \\n333145593\\nFIH\\n9229891\\n27487/f4b3\\n699211\\...\n",
       "06_506       \\n333145593\\nFIH\\n9229891\\n50096/c65m\\n571634\\...\n",
       "...                                                        ...\n",
       "18_2_123244  Admission Date:  [**2100-11-30**]             ...\n",
       "18_2_107058  Admission Date:  [**2178-3-12**]              ...\n",
       "18_2_114923  Admission Date:  [**2132-1-30**]              ...\n",
       "18_2_103074  Admission Date:  [**2125-10-10**]       Discha...\n",
       "18_2_101276  Admission Date:  [**2120-10-4**]              ...\n",
       "\n",
       "[7901 rows x 1 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_years = []\n",
    "for _, year in EHR_years.items():\n",
    "    df_years.append(pd.DataFrame.from_dict(year, orient=\"index\"))\n",
    "df = pd.concat(df_years)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fed4f1-ee05-48b1-97b4-05579d7c19b9",
   "metadata": {},
   "source": [
    "## Part 2 - Getting rid of doubles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5b151566-24b5-44eb-8a71-54661b0623f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Go through item in DATA folder\n",
    "\n",
    "root = \"Data/ALL\"\n",
    "\n",
    "ls = functions.search_for_files(root, \".txt\")\n",
    "ALL = {}\n",
    "for file in ls:\n",
    "    ID = file.split(\"/\")[2][:-4]\n",
    "    with open(file, encoding=\"utf-8-sig\") as f:\n",
    "        txt = \"\".join(f.readlines())\n",
    "    txt = txt.replace(\"\\n\", \"\")\n",
    "    txt = txt.lower()\n",
    "    txt = txt.strip()\n",
    "    txt = txt.replace(\"*\", \"\")\n",
    "    txt = txt.replace(\"[ report_end ]\", \"\")\n",
    "    ALL[ID] = txt\n",
    "\n",
    "ALL = dict(sorted(ALL.items()))\n",
    "ALL_c = ALL.copy()\n",
    "# len(ALL_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0ca02aa0-8412-4e83-9788-3904263940e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1\n",
    "for k, v in ALL.items():\n",
    "    if v in list(ALL.values())[a:]:\n",
    "        del ALL_c[k]\n",
    "    a += 1\n",
    "\n",
    "ALL = ALL_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "97439298-7751-4fd2-9d7e-05e897e61ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = {}\n",
    "i = 1\n",
    "\n",
    "for idx, txt in ALL.items():\n",
    "    final[i] = txt\n",
    "    i += 1\n",
    "\n",
    "df_all = pd.DataFrame.from_dict(final, orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "25e2a901-0d80-4aab-a676-82304af9317a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all of the individual EHR files in one folder on the computer\n",
    "# Remove all files in the folder in a first place\n",
    "# for f in glob.glob('Data/ALL/*'):\n",
    "#     os.remove(f)\n",
    "\n",
    "# # Add all the folders\n",
    "# for idx, txt in final.items():\n",
    "#     if not os.path.exists(f'Data/ALL/{idx}.txt'):\n",
    "#         cf.new_file('Data/ALL',idx,txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3d1fb80e-6439-4660-9cdc-162291022378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRs = []\n",
    "# for i in [EHR_06,EHR_08, EHR_09_bis, EHR_10, EHR_11, EHR_12,  EHR_14, EHR_18_1,  EHR_18_2]:\n",
    "#     #print('Test ', len(i.values())-len(set(i.values())))\n",
    "#     CRs += [j for j in i.values()]\n",
    "# len(set(CRs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cbfb39fe-1193-4a99-9472-4ad653fe74d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# EHR_years = [EHR_06,EHR_08, EHR_09_bis, EHR_10, EHR_11, EHR_12,  EHR_14, EHR_18_1,  EHR_18_2]\n",
    "# EHR_copy = [EHR.copy() for EHR in EHR_years]\n",
    "# for i in range(len(EHR_years)):\n",
    "#     for j in range(len(EHR_years)):\n",
    "#         if EHR_years[i] == EHR_years[j]:\n",
    "#             pass\n",
    "#         else:\n",
    "#             for idx, txt in EHR_years[i].items():\n",
    "#                 for idx_2, txt_2 in EHR_years[j].items():\n",
    "#                     if txt == txt_2:\n",
    "#                         if idx_2 in EHR_copy[j]:\n",
    "#                             del EHR_copy[j][idx_2]\n",
    "# print(sum([len(x) for x in EHR_copy]),[len(x) for x in EHR_years])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7002ac6f-5977-4b5f-978c-8f99c8f29814",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Part 2 - Clean the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22ba3b0d-a0a9-4c19-9a00-db4165a6f65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/gaultierjs/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/gaultierjs/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/gaultierjs/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/gaultierjs/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"words\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23884ec6-82d4-4062-b11d-b43092ba1cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "words = set(nltk.corpus.words.words())\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9756fa-45ff-44da-8129-b6626daf6e9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JS_venv_data_science_1",
   "language": "python",
   "name": "js_venv_data_science_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
