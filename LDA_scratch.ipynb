{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.usetex = True\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'Data/2-1_cleaned_corpus_rapports.csv'\n",
    "data = pd.read_csv(file, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_samples = data['rapport'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=10000,\n",
    "                                stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = tf_vectorizer.vocabulary_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for row in tf.toarray():\n",
    "    present_words = np.where(row != 0)[0].tolist()\n",
    "    present_words_with_count = []\n",
    "    for word_idx in present_words:\n",
    "        for count in range(row[word_idx]):\n",
    "            present_words_with_count.append(word_idx)\n",
    "    docs.append(present_words_with_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = len(docs)        # number of documents\n",
    "V = len(vocabulary)  # size of the vocabulary \n",
    "T = 10               # number of topics\n",
    "\n",
    "alpha = 1 / T         # the parameter of the Dirichlet prior on the per-document topic distributions\n",
    "beta = 1 / T        # the parameter of the Dirichlet prior on the per-topic word distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8e1585f24a1420fbe5f86646e8fc27c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "z_d_n = [[0 for _ in range(len(d))] for d in docs]  # z_i_j\n",
    "theta_d_z = np.zeros((D, T))\n",
    "phi_z_w = np.zeros((T, V))\n",
    "n_d = np.zeros((D))\n",
    "n_z = np.zeros((T))\n",
    "\n",
    "## Initialize the parameters\n",
    "# m: doc id\n",
    "for d, doc in enumerate(docs):\n",
    "    # n: id of word inside document, w: id of the word globally\n",
    "    for n, w in enumerate(doc):\n",
    "        # assign a topic randomly to words\n",
    "        z_d_n[d][n] = n % T\n",
    "        # get the topic for word n in document m\n",
    "        z = z_d_n[d][n]\n",
    "        # keep track of our counts\n",
    "        theta_d_z[d][z] += 1\n",
    "        phi_z_w[z, w] += 1\n",
    "        n_z[z] += 1\n",
    "        n_d[d] += 1\n",
    "\n",
    "for iteration in tqdm(range(10)):\n",
    "    for d, doc in enumerate(docs):\n",
    "        for n, w in enumerate(doc):\n",
    "            # get the topic for word n in document m\n",
    "            z = z_d_n[d][n]\n",
    "\n",
    "            # decrement counts for word w with associated topic z\n",
    "            theta_d_z[d][z] -= 1\n",
    "            phi_z_w[z, w] -= 1\n",
    "            n_z[z] -= 1\n",
    "\n",
    "            # sample new topic from a multinomial according to our formula\n",
    "            p_d_t = (theta_d_z[d] + alpha) / (n_d[d] - 1 + T * alpha)\n",
    "            p_t_w = (phi_z_w[:, w] + beta) / (n_z + V * beta)\n",
    "            p_z = p_d_t * p_t_w\n",
    "            p_z /= np.sum(p_z)\n",
    "            new_z = np.random.multinomial(1, p_z).argmax()\n",
    "\n",
    "            # set z as the new topic and increment counts\n",
    "            z_d_n[d][n] = new_z\n",
    "            theta_d_z[d][new_z] += 1\n",
    "            phi_z_w[new_z, w] += 1\n",
    "            n_z[new_z] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: po discharge mg override interaction order qd 00 amp pain\n",
      "Topic #1: discharge patient mg day admission hospital 00 follow 10 course\n",
      "Topic #2: patient history mg day discharge chest admission pain coronary rate\n",
      "Topic #3: mg tablet po pt blood sig 10 12 11 wbc\n",
      "Topic #4: mg po qd tablet 10 pt bid daily 11 pain\n",
      "Topic #5: pain history mg chest patient normal 10 12 note year\n",
      "Topic #6: mg history day patient blood 10 12 normal time left\n",
      "Topic #7: mg daily 10 patient day po admission continue discharge tablet\n",
      "Topic #8: patient right history day left time mg discharge status blood\n",
      "Topic #9: patient mg history right artery day left disease time chest\n"
     ]
    }
   ],
   "source": [
    "inv_vocabulary = {v: k for k, v in vocabulary.items()}\n",
    "n_top_words = 10\n",
    "for topic_idx, topic in enumerate(phi_z_w):\n",
    "    message = \"Topic #%d: \" % topic_idx\n",
    "    message += \" \".join([inv_vocabulary[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "    print(message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SogLab_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e094d850592d944b38bb8613998c40358239c031dd4b8b0f43e97425e6f0245d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
